{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1: Arsenal, Aston Villa\n",
      "Processing Arsenal: Gunners\n",
      "Processing Aston Villa: AVFC\n",
      "Processing batch 2: Bournemouth, Brentford\n",
      "Processing Bournemouth: AFCBournemouth\n",
      "Processing Brentford: Brentford\n",
      "Processing batch 3: Brighton, Chelsea\n",
      "Processing Brighton: BrightonHoveAlbion\n",
      "Processing Chelsea: ChelseaFC\n",
      "Processing batch 4: Crystal Palace, Everton\n",
      "Processing Crystal Palace: CrystalPalace\n",
      "Processing Everton: Everton\n",
      "Processing batch 5: Fulham, Liverpool\n",
      "Processing Fulham: FulhamFC\n",
      "Processing Liverpool: LiverpoolFC\n",
      "Processing batch 6: Man City, Man United\n",
      "Processing Man City: MCFC\n",
      "Processing Man United: RedDevils\n",
      "Processing batch 7: Newcastle, Nottingham Forest\n",
      "Processing Newcastle: NUFC\n",
      "Processing Nottingham Forest: NFFC\n",
      "Processing batch 8: Tottenham, West Ham\n",
      "Processing Tottenham: coys\n",
      "Processing West Ham: Hammers\n",
      "Processing batch 9: Wolves, Leicester City\n",
      "Processing Wolves: Wolves\n",
      "Processing Leicester City: lcfc\n",
      "Processing batch 10: Ipswich Town, Southampton\n",
      "Processing Ipswich Town: IpswichTownFC\n",
      "Processing Southampton: SaintsFC\n",
      "All data saved to reddit_data_large.csv\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Reddit instance\n",
    "user_agent = \"Scraper 1.0 by /u/python_engineer\"\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"roVR4TckURjR22afdTBHeg\",\n",
    "    client_secret=\"EX48h9IBLKhO0U_7r7J12NbdIZXXHA\",\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Define team subreddits\n",
    "team_subreddits = {\n",
    "    \"Arsenal\": \"Gunners\",\n",
    "    \"Aston Villa\": \"AVFC\",\n",
    "    \"Bournemouth\": \"AFCBournemouth\",\n",
    "    \"Brentford\": \"Brentford\",\n",
    "    \"Brighton\": \"BrightonHoveAlbion\",\n",
    "    \"Chelsea\": \"ChelseaFC\",\n",
    "    \"Crystal Palace\": \"CrystalPalace\",\n",
    "    \"Everton\": \"Everton\",\n",
    "    \"Fulham\": \"FulhamFC\",\n",
    "    \"Liverpool\": \"LiverpoolFC\",\n",
    "    \"Man City\": \"MCFC\",\n",
    "    \"Man United\": \"RedDevils\",\n",
    "    \"Newcastle\": \"NUFC\",\n",
    "    \"Nottingham Forest\": \"NFFC\",\n",
    "    \"Tottenham\": \"coys\",\n",
    "    \"West Ham\": \"Hammers\",\n",
    "    \"Wolves\": \"Wolves\",\n",
    "    \"Leicester City\": \"lcfc\",\n",
    "    \"Ipswich Town\": \"IpswichTownFC\",\n",
    "    \"Southampton\": \"SaintsFC\"\n",
    "}\n",
    "\n",
    "# Helper function to split dictionary into chunks\n",
    "def chunk_dict(data, chunk_size):\n",
    "    items = list(data.items())\n",
    "    for i in range(0, len(items), chunk_size):\n",
    "        yield dict(items[i:i + chunk_size])\n",
    "\n",
    "# Process in batches of 2 teams\n",
    "all_posts = []\n",
    "chunk_size = 2\n",
    "for batch_idx, batch in enumerate(chunk_dict(team_subreddits, chunk_size), 1):\n",
    "    print(f\"Processing batch {batch_idx}: {', '.join(batch.keys())}\")\n",
    "    for team, subreddit in batch.items():\n",
    "        print(f\"Processing {team}: {subreddit}\")\n",
    "        try:\n",
    "            for submission in reddit.subreddit(subreddit).rising(limit=100):\n",
    "                submission.comments.replace_more(limit=100)  # Partial comment expansion\n",
    "                comments_with_replies = []\n",
    "\n",
    "                for comment in submission.comments.list()[:100]:\n",
    "                    replies = [reply.body for reply in comment.replies[:10]]  # Capture top replies\n",
    "                    \n",
    "                    comments_with_replies.append({\n",
    "                        'comment': comment.body,\n",
    "                        'replies': replies\n",
    "                    })\n",
    "\n",
    "                post = {\n",
    "                    'team': team,\n",
    "                    'title': submission.title,\n",
    "                    'selftext': submission.selftext if submission.selftext else \"\",\n",
    "                    'comments': comments_with_replies\n",
    "                }\n",
    "                all_posts.append(post)\n",
    "                time.sleep(0.15)  # Prevent hitting rate limits\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {team}: {e}\")\n",
    "            time.sleep(0.15)\n",
    "\n",
    "# Flatten and save\n",
    "flat_posts = []\n",
    "for post in all_posts:\n",
    "    for comment_data in post['comments']:\n",
    "        flat_posts.append({\n",
    "            'team': post['team'],\n",
    "            'title': post['title'],\n",
    "            'selftext': post['selftext'],\n",
    "            'comment': comment_data['comment'],\n",
    "            'replies': \" \".join(comment_data['replies'])\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "df_all = pd.DataFrame(flat_posts)\n",
    "df_all.to_csv(\"../data_raw/reddit_rising.csv\", index=False)\n",
    "\n",
    "print(\"All data saved to reddit_rising.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
